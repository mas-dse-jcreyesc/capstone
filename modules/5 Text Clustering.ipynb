{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Clustering Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please be aware that this notebook inherits functions located in scripts contained in the `api` directory. You may have to change paths to reflect your local directory structure. Also be wary of passing in the correct path to the data CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import pdb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from api.text_operations import (\n",
    "    clean_data,\n",
    "    encode_text\n",
    ")\n",
    "from api.clustering import create_clusters\n",
    "\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'text_clusters.csv'\n",
    "tname = 'text_samples.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(fname) or not os.path.exists(tname):\n",
    "    #--------------------------------------------------------------------------#\n",
    "    # Load data\n",
    "    #--------------------------------------------------------------------------#\n",
    "    data = pd.read_csv('./SampleData.csv')\n",
    "    print(data.shape)\n",
    "    \n",
    "    # Clean NaNs\n",
    "    txt_cols = [\n",
    "        'OrganizationName',\n",
    "        '/IRS990/Desc',\n",
    "        '/IRS990/ActivityOrMissionDesc',\n",
    "        '/IRS990/MissionDesc'\n",
    "    ]\n",
    "    data.dropna(subset=txt_cols, inplace=True)\n",
    "    print(data.shape)\n",
    "    \n",
    "    \n",
    "    #--------------------------------------------------------------------------#\n",
    "    # Clean text data\n",
    "    #--------------------------------------------------------------------------#\n",
    "    clean_df = clean_data(data)\n",
    "\n",
    "    \n",
    "    #--------------------------------------------------------------------------#\n",
    "    # Get text encodings\n",
    "    #--------------------------------------------------------------------------#\n",
    "    print()\n",
    "    encoding_dict = encode_text(clean_df)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    #--------------------------------------------------------------------------#\n",
    "    # Assemble text encodings\n",
    "    #--------------------------------------------------------------------------#\n",
    "    X = np.array([encoding_dict[r['EIN']] for _, r in clean_df.iterrows()])\n",
    "    print(X.shape)\n",
    "    \n",
    "    \n",
    "    #--------------------------------------------------------------------------#\n",
    "    # Dimensionality reduction (clustering)\n",
    "    #--------------------------------------------------------------------------#\n",
    "    pca_dimr = PCA(n_components=200)\n",
    "    X_dimr = pca_dimr.fit_transform(X)\n",
    "    expvar_1 = sum(pca_dimr.explained_variance_ratio_)\n",
    "    print('\\nExplained variance (clustering): %.4f'%expvar_1)\n",
    "    \n",
    "    \n",
    "    #--------------------------------------------------------------------------#\n",
    "    # Get cluster results\n",
    "    #--------------------------------------------------------------------------#\n",
    "    Z, gap_metrics, bc, clusters = create_clusters(X_dimr, C=500)\n",
    "    clean_df['cluster'] = clusters\n",
    "    \n",
    "    \n",
    "    #--------------------------------------------------------------------------#\n",
    "    # Get cluster centroids\n",
    "    #--------------------------------------------------------------------------#\n",
    "    centroids = np.zeros((bc[0], X_dimr.shape[1]))\n",
    "    grouped_dfs = clean_df.groupby(by='cluster')\n",
    "    for label, indexes in grouped_dfs.groups.items():\n",
    "        centroids[label-1] = X_dimr[indexes].mean(axis=0)\n",
    "    \n",
    "    \n",
    "    #--------------------------------------------------------------------------#\n",
    "    # Get cluster distances for each sample\n",
    "    #--------------------------------------------------------------------------#\n",
    "    dist_dict = {'cluster_%d'%(i+1):[] for i in range(centroids.shape[0])}\n",
    "\n",
    "    for embed in X_dimr:\n",
    "        for i, centroid in enumerate(centroids):\n",
    "            dist_dict['cluster_%d'%(i+1)].append(euclidean(embed, centroid))\n",
    "    \n",
    "    \n",
    "    #--------------------------------------------------------------------------#\n",
    "    # Integrate distances with cluster dataframe\n",
    "    #--------------------------------------------------------------------------#\n",
    "    dist_df = pd.DataFrame.from_dict(dist_dict, orient='columns')\n",
    "    dist_df = dist_df[sorted(dist_df.columns, key=lambda x: int(x.split('_')[-1]))]\n",
    "\n",
    "    cluster_df = pd.concat([clean_df, dist_df], axis=1)\n",
    "    print(cluster_df.shape)\n",
    "    \n",
    "    \n",
    "    #--------------------------------------------------------------------------#\n",
    "    # Drop unnecessary columns and export\n",
    "    #--------------------------------------------------------------------------#\n",
    "    drop_cols = ['Name', 'Text']\n",
    "    cluster_df.drop(labels=drop_cols, axis=1, inplace=True)\n",
    "    cluster_df.to_csv(fname, index=False)\n",
    "    clean_df.to_csv(tname, index=False)\n",
    "else:\n",
    "    cluster_df = pd.read_csv(fname)\n",
    "    clean_df = pd.read_csv(tname)\n",
    "#------------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cluster dataframe\n",
    "cluster_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned text dataframe\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------#\n",
    "def return_topics(model, feature_names, no_top_words):\n",
    "    \"\"\"\n",
    "    Function for returning the top words of a model's topics\n",
    "    \"\"\"\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        w = [feature_names[i] for i in topic.argsort()[:-no_top_words-1:-1]]\n",
    "        topic_dict[topic_idx] = w\n",
    "    \n",
    "    return topic_dict\n",
    "\n",
    "\n",
    "def get_lda_topics(clean_df):\n",
    "    \"\"\"\n",
    "    Function for converting a cleaned text dataframe (with a cluster ID column) \n",
    "    into a dataframe specifying the LDA topic words corresponding to each \n",
    "    cluster ID\n",
    "    \"\"\"\n",
    "    assert 'cluster' in clean_df.columns\n",
    "    \n",
    "    tf_params = {\n",
    "        'max_df': 0.95,\n",
    "        'min_df': 2,\n",
    "        'max_features': 1000,\n",
    "        'stop_words': 'english'\n",
    "    }\n",
    "    lda_params = {\n",
    "        'n_topics': 1, \n",
    "        'max_iter': 5, \n",
    "        'learning_method': 'online', \n",
    "        'learning_offset': 50,\n",
    "        'random_state': 0\n",
    "    }\n",
    "    \n",
    "    topic_dict = {}\n",
    "    \n",
    "    min_id = np.min(clean_df['cluster'])\n",
    "    max_id = np.max(clean_df['cluster'])\n",
    "    \n",
    "    for i in np.arange(min_id, max_id+1):\n",
    "        subset = clean_df[clean_df['cluster']==i]\n",
    "        \n",
    "        # Fit count vectorizer\n",
    "        tf_vectorizer = CountVectorizer(**tf_params)\n",
    "        tf = tf_vectorizer.fit_transform(subset['Text'])\n",
    "        tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "        \n",
    "        # Run LDA\n",
    "        lda = LatentDirichletAllocation(**lda_params).fit(tf)\n",
    "        \n",
    "        # Get top words for topic\n",
    "        wd = return_topics(lda, tf_feature_names, 10)\n",
    "        topic_dict[i] = wd[0]\n",
    "    \n",
    "    # Convert topic dict to dataframe\n",
    "    topic_df = pd.DataFrame.from_dict(topic_dict, orient='columns')\n",
    "    topic_df.columns = ['topic_%s'%c for c in topic_df.columns]\n",
    "    \n",
    "    return topic_df\n",
    "#------------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic dataframe\n",
    "td = get_lda_topics(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export topic dataframe\n",
    "td.to_csv('lda_topics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual validation\n",
    "for i, row in clean_df[clean_df['cluster']==1].iterrows():\n",
    "    print('\\n\\n' + row['Text'] + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
