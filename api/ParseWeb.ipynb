{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs  \n",
    "from urllib.request import urlopen, Request, urlopen\n",
    "from urllib.parse import urlparse,ParseResult\n",
    "import re\n",
    "import heapq \n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParseWeb():\n",
    "    \n",
    "    def __init__(self, verbose=True, debug=False, printSites=False):\n",
    "        \n",
    "        self.printSites = printSites\n",
    "        self.verbose = verbose\n",
    "        self.debug = debug\n",
    "        \n",
    "        if self.verbose:\n",
    "            self.printSites = True\n",
    "        if self.debug:\n",
    "            self.verbose = True\n",
    "            self.printSites = True\n",
    "            \n",
    "        self.headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.3'}\n",
    "            \n",
    "        \n",
    "    def text( self, url ):\n",
    "        \n",
    "        article_text = \"\"\n",
    "        links = []\n",
    "        \n",
    "        if type(url) is str:\n",
    "            # first time we see this URL\n",
    "            \n",
    "            if not url or not str(url) or len(url.split('.')) < 2:\n",
    "                if self.verbose: print( 'Not valid URL [%s]' % url )\n",
    "                return ''\n",
    "\n",
    "            sites = self.format_url( url )\n",
    "\n",
    "\n",
    "            # Get the primary page and any good link\n",
    "            for site in sites:\n",
    "                text, links = self.extract_text( site, links=True )\n",
    "                article_text += text\n",
    "                \n",
    "        else:\n",
    "            # update to the entry\n",
    "            sites = url\n",
    "            \n",
    "        # If we got links then try those too ( only from base path )\n",
    "        for x in links:\n",
    "            if '@' in x:\n",
    "                continue\n",
    "            elif self.test_url( x ):\n",
    "                sites.append( x )\n",
    "                article_text += self.extract_text( x )\n",
    "            elif self.test_url( site + x ):\n",
    "                sites.append( site + x )\n",
    "                article_text += self.extract_text( site + x )\n",
    "            elif self.test_url( site + '/' + x ):\n",
    "                sites.append( site + '/' + x )\n",
    "                article_text += self.extract_text( site + '/' + x )\n",
    "                \n",
    "\n",
    "        if self.printSites: print( sites )\n",
    "        if self.debug: print( article_text )\n",
    "            \n",
    "        if not article_text:\n",
    "            if self.verbose: print('ERROR: no text came out of website: %s' % sites )\n",
    "        \n",
    "        if article_text:\n",
    "            return sites, self.parse_text( article_text )\n",
    "        else:\n",
    "            return sites, ''\n",
    "\n",
    "        \n",
    "    def extract_text( self, url, links=False ):\n",
    "        if self.debug: print( 'READ: %s' % url )\n",
    "            \n",
    "        article_text = \"\"\n",
    "        validlinks = []\n",
    "\n",
    "        article = urlopen(Request(url, headers=self.headers),  timeout=4).read()\n",
    "        parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "        paragraphs = parsed_article.find_all('p')\n",
    "        for p in paragraphs:  \n",
    "            article_text += p.text\n",
    "            \n",
    "        if self.debug: print( article_text )\n",
    "\n",
    "        if links:\n",
    "            for l in parsed_article.findAll('a', attrs={'href': re.compile('(about|mission|description|info)')}):\n",
    "                #if self.debug: print( 'Found link: %s' % l.get('href') )\n",
    "                validlinks.append(l.get('href'))\n",
    "            \n",
    "            return article_text, set(validlinks)\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            return article_text\n",
    "\n",
    "\n",
    "    def test_url( self, url ):\n",
    "        try:\n",
    "            if self.debug:  print( 'Request HEAD from URL: %s' % url )\n",
    "            req = urlopen(Request(url, method='HEAD', headers=self.headers),  timeout=4)\n",
    "            if req.getcode() < 400 :\n",
    "                if self.debug:  print( 'Found: %s %s' % (url,req.getcode()) )\n",
    "                return True\n",
    "            else:\n",
    "                if self.debug: \n",
    "                    print( 'ERROR: %s' % req.getcode() )\n",
    "                    print( 'ERROR: returned %s' % req )\n",
    "        except:\n",
    "            if self.debug: print( 'Failure on request' )\n",
    "        \n",
    "        return False\n",
    "        \n",
    "\n",
    "    def format_url(self, url):\n",
    "\n",
    "        urllist = []\n",
    "\n",
    "        p = url.lower().strip()\n",
    "        \n",
    "        for bad in ['http://', 'https://', 'www.']:\n",
    "            p = p.replace( bad, '' )\n",
    "            \n",
    "        p = urlparse( p )\n",
    "        \n",
    "        if self.debug: print( p )\n",
    "\n",
    "        netloc = p.netloc or p.path\n",
    "        path = p.path if p.netloc else ''\n",
    "\n",
    "        for method in ['http', 'https']:\n",
    "            test = ParseResult(method, netloc, path, *p[3:]).geturl()\n",
    "\n",
    "            if self.test_url( test ):\n",
    "                if self.debug: print( 'Add %s to list' % test )\n",
    "                urllist.append( test )\n",
    "\n",
    "            if not urllist and not 'www' in test :\n",
    "                test = ParseResult(method, 'www.'+netloc, path, *p[3:]).geturl()\n",
    "                if self.test_url( test ):\n",
    "                    if self.debug: print( 'Add %s to list' % test )\n",
    "                    urllist.append( test )\n",
    "                \n",
    "        # If found then get the last only\n",
    "        if urllist:\n",
    "            return urllist[-1:]\n",
    "        \n",
    "        return urllist\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def parse_text( self, article_text ):\n",
    "\n",
    "        # Removing Square Brackets and Extra Spaces\n",
    "        article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)  \n",
    "        article_text = re.sub(r'\\s+', ' ', article_text)  \n",
    "\n",
    "        # Removing special characters and digits\n",
    "        formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )  \n",
    "        formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)  \n",
    "\n",
    "        sentence_list = nltk.sent_tokenize(article_text)  \n",
    "\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "        word_frequencies = {}  \n",
    "        for word in nltk.word_tokenize(formatted_article_text):  \n",
    "            if word not in stopwords:\n",
    "                if word not in word_frequencies.keys():\n",
    "                    word_frequencies[word] = 1\n",
    "                else:\n",
    "                    word_frequencies[word] += 1\n",
    "\n",
    "        maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "        for word in word_frequencies.keys():  \n",
    "            word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "\n",
    "        sentence_scores = {}  \n",
    "        for sent in sentence_list:  \n",
    "            for word in nltk.word_tokenize(sent.lower()):\n",
    "                if word in word_frequencies.keys():\n",
    "                    if len(sent.split(' ')) < 30:\n",
    "                        if sent not in sentence_scores.keys():\n",
    "                            sentence_scores[sent] = word_frequencies[word]\n",
    "                        else:\n",
    "                            sentence_scores[sent] += word_frequencies[word]\n",
    "\n",
    "        summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "        summary = ' '.join(summary_sentences)  \n",
    "        if self.debug: print(summary)  \n",
    "\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#------------------------------------------------------------------------------#\n",
    "# Test function\n",
    "#------------------------------------------------------------------------------#\n",
    "if False:\n",
    "    \"\"\"\n",
    "    Tests class to parse website data\n",
    "    \"\"\"\n",
    "    \n",
    "    #print( ' Start Test ParseWeb() ')\n",
    "    url = 'www.ymca.net'\n",
    "    #url = 'www.noahcdc.org'\n",
    "    #url = 'releafmichigan.org'\n",
    "\n",
    "    parser = ParseWeb( verbose=False, debug=False, printSites=False )\n",
    "    \n",
    "    websites, text = parser.text( url )\n",
    "    print( websites ) \n",
    "    print( text ) \n",
    "    \n",
    "    \n",
    "#------------------------------------------------------------------------------#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
